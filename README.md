# Proyecto de predicci√≥n de ventas y clasificaci√≥n de productos en diferentes cl√∫ster, seg√∫n diferentes caracter√≠sticas


<p align="center">
   <img align="center" width="200" src="https://raw.githubusercontent.com/Sergiochueco-94/DSmarket_cluster_y_prediction/main/img/DsMarket.PNG" />
</p>

 <p align="left">
   <img src="https://img.shields.io/badge/STATUS-EN%20DESAROLLO-green">
</p>

version https://git-lfs.github.com/spec/v1
oid sha256:7faffe61a084669fa5295aaca8150a5e89df0cc0acb9711bbd7cf2e0ddf7b592
size 5005

## Tecnolog√≠as utilizadas:

 - Python
 - Pandas
 - Numpy
 - XGBoost
 - KMEANS
 - Scikit-learn
 - Google Colab
 - Visual Studio Code
 - Jupyter Notebook
 - Git
 - Github


## Descripci√≥n proyecto

Partimos de una BBDD compuesta de 3 archivos csv:

- daily_calendar_with_events.csv
- item_sales.csv
- item_prices

De aqu√≠ generaremos varios archivos csv m√°s para poder utilizar una presentaci√≥n en Power BI.

Una vez explicado esto, en este proyecto se busca poder categorizar los productos en **diferentes grupos(cl√∫ster)** para poder identificar sus principales caracter√≠sticas y poder tomar mejores decisiones a nivel de negocio bas√°ndonos en esto.

Por otro lado, tambi√©n se busca **predecir las ventas** de los 28 d√≠as siguientes a la √∫ltima fecha.

Para poder conseguir todo esto, el proyecto se divide en varias partes:

1) Crear un √∫nico dataframe en el que poder recoger toda la informaci√≥n necesaria que contienen los 3 CSV distinto.

   Para esto se han utilizado varias t√©cnicas como se puede observar en el c√≥digo.
   Tambi√©n aclarar que en el dataframe final(df_semanal_fechas), se imputa el valor anterior o posterior (bfill, ffill) a los precios con nulos, asumiendo que es parecido a lo posterior/anterior, ya que hablamos de una semana de diferencia en la que el precio puede que no sea muy diferente.

   El archivo que se ha originado es df_final_fechas_14102023.csv. Este archivo ha sido generado con un .ipynb en la carpeta /src llamado JOIN_DF.ipynb. Dicho archivo ha tenido que ser ejecutado en Google Colab Pro, ya que consume +25GB de RAM, debido al tama√±o de los  dataframe. Utilizamos Google Colab Pro, para poder hacer frente a esta demanda de RAM. 
   Si tienes una RAM de +30GB no tendr√°s problema en ejecutar todo el proyecto en local, para poder hacer cualquier comprobaci√≥n o modificaci√≥n.
   
   Una vez ejecutado todo en colab se convierte el dataframe resultante a CSV y se descarga para poder luego trabajar con este archivo en local y poder realizar el EDA y los diferentes modelos del proyecto.

2) Una vez tenemos esto, pasamos a crear un modelo de clusterizaci√≥n en el que poder clasificar los productos. Para ellos utilizaremos el algoritmo KMEANS, calcularemos mediante la curva del codo y las variables que sacamos, cu√°l es la cantidad √≥ptima de cl√∫ster.

3) En este punto, generamos otro modelo a trav√©s de el algoritmo XGBoostRegressor, con el que pretendemos predecir la venta durante los pr√≥ximos 28 d√≠as. Evaluaremos como de buenos es nuestro modelo modelo con diferentes m√©tricas como el RMSE para ver cu√°l es el margen de error que tiene..



Tenemos diversas carpetas:

 - **CSV**: donde tenemos los CSV utilizado para obtner los datos de trabajo (item_prices.csv,item_sales.csv, daily_calendar_with_events.csv). El resto de CSV han sido creado dentro de los jupyter notebook para poder exportar en esta caso la informaci√≥n a un Power BI y poder hacer una presentaci√≥n a negocio. Sobre diferntes insights y tambi√©n sobre el funcionamiento de nuestros 2 modelos (predicci√≥n de ventas y clustering de tipos de productos)

 - **img**: compuesta por imagenes para el readme.

 - **models**: contiene archivos .pkl que contienen los modelos generados para poder ponerlos en producci√≥n.

 - **src**:  contiene 4 archivos .ipynb. Clustering.ipynb como el propio nombre indica es nuestro notebook donde hemos entrenado el modelo de clustering. Por otro lado, el Predict_ventas.ipynb es nuestro nuestro notebook donde hemos entrenado el modelo de predicci√≥n de ventas para el mes siguiente.

   Tambi√©n tenemos dos archivos m√°s, uno de EDA.ipynb donde realizamos un peque√±o EDA de nuestros datos para decidir como entrenar despu√©s a nuestros modelos y ver tambi√©n como se distribuye nuestros datos sacando insights para el negocio.
   Por √∫ltimo, esta carpea contiene el archivo JOIN_DF donde se realiza la limpieza, organizaci√≥n e uni√≥n de los 3 archivos de datos con los que empezamos inicialmente, para poder trabajar sobre una √∫nica base de datos. Recientemente, se ha a√±adido otro notebook que convierte el csv limpio en formato JSON.

- **JSON**: contiene los datos ya limpios, estructurados y unificados del proyecto para poder utilizarlos en una posible implementaci√≥n en web.


## üõ†Ô∏è Este proyecto est√° actualmente en proceso de terminar... se ir√° a√±adiendo m√°s informaci√≥n y m√°s c√≥digo conforme se vaya avanzando... üõ†Ô∏è

- [x] Extracci√≥n, limpieza, transformaci√≥n de datos
- [x] Archivos a formato JSON
- [x] Entrenamiento de modelo de Machine Learning (Predicci√≥n de ventas y Clustering de productos)
- [ ] Dockerfile
- [ ] Uso de Airflow, MLflow
- [ ] Puesta en producci√≥n
